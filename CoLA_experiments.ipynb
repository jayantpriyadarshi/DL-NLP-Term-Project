{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CoLA_experiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3041e54728bd491092e52eae1c85c706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_59bbf763b75b4fb7b61008f007902547",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3feb69cd91e24391b099844521ad6554",
              "IPY_MODEL_ec1478e422aa4083a1c8c28e35c85162"
            ]
          }
        },
        "59bbf763b75b4fb7b61008f007902547": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3feb69cd91e24391b099844521ad6554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_250f5d55378c4864bbe0e060afc8aa9a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_45d2e6d19cb74294970f63d1fe7845f6"
          }
        },
        "ec1478e422aa4083a1c8c28e35c85162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a02f655a32094cf8bb25e0ac95dcc088",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 8.35MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d91c5bd95c8481b9dfdc9ec82f4ea0f"
          }
        },
        "250f5d55378c4864bbe0e060afc8aa9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "45d2e6d19cb74294970f63d1fe7845f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a02f655a32094cf8bb25e0ac95dcc088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3d91c5bd95c8481b9dfdc9ec82f4ea0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c0e602d1b624a79ad5e5cf0b6e5cdc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_adea9e7f5bde4a028cd446b28d4376ee",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e20f259efdab47839dee5a2d995af346",
              "IPY_MODEL_1e293e9fc2d74e4194f25ab3ec659e3f"
            ]
          }
        },
        "adea9e7f5bde4a028cd446b28d4376ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e20f259efdab47839dee5a2d995af346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6e6d31f1f8c3435e94a103d8b0652184",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c5839b54c0f466b84cb95b26edaa81f"
          }
        },
        "1e293e9fc2d74e4194f25ab3ec659e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_817961b2b4d74c1882b09c9018c5cec6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 4.76MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c12fc260cb3543e1ae361d8f9218fc16"
          }
        },
        "6e6d31f1f8c3435e94a103d8b0652184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c5839b54c0f466b84cb95b26edaa81f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "817961b2b4d74c1882b09c9018c5cec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c12fc260cb3543e1ae361d8f9218fc16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c917799c7124a63b709de88fd7235f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fdf154e2dc274b62ad5e7f8f104922bb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_55434cbb2ead4393809f11aaff779687",
              "IPY_MODEL_950b25117a2745bd8135b7382cf3689b"
            ]
          }
        },
        "fdf154e2dc274b62ad5e7f8f104922bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "55434cbb2ead4393809f11aaff779687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8bdcac8cbb504d9da2794bd875a05928",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 798011,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 798011,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2eca023a17a04e1cad1c63d9126e6250"
          }
        },
        "950b25117a2745bd8135b7382cf3689b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_897bb4e74b754aba884da7fd35a18bc7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 798k/798k [00:00&lt;00:00, 5.46MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_008c1593528c440e993c00262103131e"
          }
        },
        "8bdcac8cbb504d9da2794bd875a05928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2eca023a17a04e1cad1c63d9126e6250": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "897bb4e74b754aba884da7fd35a18bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "008c1593528c440e993c00262103131e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XJ3xohz5WYb",
        "outputId": "4b30941f-6a83-4bd2-8987-3190d52c629c"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import string\r\n",
        "from statistics import mean\r\n",
        " \r\n",
        "import copy\r\n",
        "import os\r\n",
        "import re\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim\r\n",
        "import nltk\r\n",
        " \r\n",
        "from nltk.corpus import stopwords\r\n",
        "import spacy\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, WeightedRandomSampler\r\n",
        "import torch.nn.functional as F\r\n",
        " \r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import matthews_corrcoef\r\n",
        " \r\n",
        "!pip install transformers\r\n",
        "!pip install pytorch-transformers\r\n",
        " \r\n",
        "import transformers\r\n",
        "from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\r\n",
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\r\n",
        "from transformers import RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\r\n",
        "from transformers import AdamW\r\n",
        " \r\n",
        "from tqdm import tqdm, trange\r\n",
        " "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/40/866cbfac4601e0f74c7303d533a9c5d4a53858bd402e08e3e294dd271f25/transformers-4.2.1-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 18.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 50.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 48.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=8a2fbfe26caa05fd49c24ea5cba5b69ec0132c7c95e572247713e204aee6b84d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.1\n",
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 16.8MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/ef/8fd72f02fd605b5a9390c31def4f266978ca6d8e79f3ba740491e487044b/boto3-1.16.57-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.0.43)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.7.0+cu101)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.57\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/70/35b0627ec7c38cdac30636b98549075cb6a8f3ff236a26fa204f6b509ded/botocore-1.19.57-py2.py3-none-any.whl (7.2MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2MB 49.9MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 12.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.57->boto3->pytorch-transformers) (2.8.1)\n",
            "\u001b[31mERROR: botocore 1.19.57 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, sentencepiece, pytorch-transformers\n",
            "Successfully installed boto3-1.16.57 botocore-1.19.57 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.3.4 sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaB94Bxl5xkU",
        "outputId": "6ec33c9b-7bd2-4c9d-91c7-89ac96d1e38d"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrcGDxaY5knk"
      },
      "source": [
        "def clean_data(x):\r\n",
        "    puncts = []\r\n",
        "    # puncts = puncts.replace(\"-\", \"\")\r\n",
        "    # stop_words = set(stopwords.words('english'))\r\n",
        "    sentences = []\r\n",
        "    # x = [''.join(c.lower() for c in s.replace(\"-\", \" \") if c not in puncts) for s in x]\r\n",
        "    x = [''.join(c.lower() for c in s if c not in puncts) for s in x]\r\n",
        "    # x = [''.join(c for c in str(s)[1:-4].replace(\"-\", \" \") if c not in puncts) for s in x]\r\n",
        "    for sent in x:\r\n",
        "        # text_no_nums = re.sub(r'\\d+', '', sent)\r\n",
        "        # text_no_doublespace = re.sub('\\s+', ' ', text_no_nums).strip()\r\n",
        "        sentences.append(sent)\r\n",
        "    return sentences  \r\n",
        "\r\n",
        "def preprocess_targets(y):\r\n",
        "    dic = {'NEG': 1, 'POS': 0}\r\n",
        "    for t in range(len(y)):\r\n",
        "        y[t] = dic[y[t]]\r\n",
        "    y = torch.tensor(y, dtype=torch.long)   \r\n",
        "    return y  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZODSR0rY5t4K",
        "outputId": "e3fc7696-e2f0-4e75-a7a8-9422b093e476"
      },
      "source": [
        "path = \"/content/drive/MyDrive/third_sem/DL_NLP/Project/dataset/CoLA/\"\r\n",
        "\r\n",
        "train_path = path + 'train.tsv'\r\n",
        "val_path = path + 'dev.tsv'\r\n",
        "test_path = path + 'test.tsv'\r\n",
        "\r\n",
        "df = pd.read_csv(train_path, sep='\\t', names=['sentence', 'sent_source', 'label', 'label_source'])\r\n",
        "print(df[:5])\r\n",
        "x_train = df[\"label_source\"].values\r\n",
        "y_train = df[\"sent_source\"].values\r\n",
        "print(\"hh\",x_train[:10])\r\n",
        "print(y_train[:100])\r\n",
        "\r\n",
        "df = pd.read_csv(val_path, sep='\\t', names=['sentence', 'sent_source', 'label', 'label_source'])\r\n",
        "x_val = df[\"label_source\"].values\r\n",
        "y_val = df[\"sent_source\"].values\r\n",
        "\r\n",
        "print(x_val[:5])\r\n",
        "\r\n",
        "# x_val = clean_data(x_val)\r\n",
        "# print(\"hereee\",x_val[:2])\r\n",
        "# # x_train = [x + \" [SEP] [CLS]\" for x in x_train]\r\n",
        "\r\n",
        "# df = pd.read_csv(test_path, sep='\\t')\r\n",
        "# x_test = df[\"sentence\"].values\r\n",
        "# print(df)\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  sentence  ...                                       label_source\n",
            "0     gj04  ...  Our friends won't buy this analysis, let alone...\n",
            "1     gj04  ...  One more pseudo generalization and I'm giving up.\n",
            "2     gj04  ...   One more pseudo generalization or I'm giving up.\n",
            "3     gj04  ...     The more we study verbs, the crazier they get.\n",
            "4     gj04  ...          Day by day the facts are getting murkier.\n",
            "\n",
            "[5 rows x 4 columns]\n",
            "hh [\"Our friends won't buy this analysis, let alone the next one we propose.\"\n",
            " \"One more pseudo generalization and I'm giving up.\"\n",
            " \"One more pseudo generalization or I'm giving up.\"\n",
            " 'The more we study verbs, the crazier they get.'\n",
            " 'Day by day the facts are getting murkier.' \"I'll fix you a drink.\"\n",
            " 'Fred watered the plants flat.'\n",
            " 'Bill coughed his way out of the restaurant.'\n",
            " \"We're dancing the night away.\" 'Herman hammered the metal flat.']\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1]\n",
            "['The sailors rode the breeze clear of the rocks.'\n",
            " 'The weights made the rope stretch over the pulley.'\n",
            " 'The mechanical doll wriggled itself loose.'\n",
            " 'If you had eaten more, you would want less.'\n",
            " 'As you eat the most, you want the least.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163,
          "referenced_widgets": [
            "3041e54728bd491092e52eae1c85c706",
            "59bbf763b75b4fb7b61008f007902547",
            "3feb69cd91e24391b099844521ad6554",
            "ec1478e422aa4083a1c8c28e35c85162",
            "250f5d55378c4864bbe0e060afc8aa9a",
            "45d2e6d19cb74294970f63d1fe7845f6",
            "a02f655a32094cf8bb25e0ac95dcc088",
            "3d91c5bd95c8481b9dfdc9ec82f4ea0f",
            "6c0e602d1b624a79ad5e5cf0b6e5cdc0",
            "adea9e7f5bde4a028cd446b28d4376ee",
            "e20f259efdab47839dee5a2d995af346",
            "1e293e9fc2d74e4194f25ab3ec659e3f",
            "6e6d31f1f8c3435e94a103d8b0652184",
            "6c5839b54c0f466b84cb95b26edaa81f",
            "817961b2b4d74c1882b09c9018c5cec6",
            "c12fc260cb3543e1ae361d8f9218fc16",
            "3c917799c7124a63b709de88fd7235f1",
            "fdf154e2dc274b62ad5e7f8f104922bb",
            "55434cbb2ead4393809f11aaff779687",
            "950b25117a2745bd8135b7382cf3689b",
            "8bdcac8cbb504d9da2794bd875a05928",
            "2eca023a17a04e1cad1c63d9126e6250",
            "897bb4e74b754aba884da7fd35a18bc7",
            "008c1593528c440e993c00262103131e"
          ]
        },
        "id": "T8h74crB_O2T",
        "outputId": "edbe6f58-b508-4878-beae-9ecaee0aa02b"
      },
      "source": [
        "tokenizer_bert = RobertaTokenizer.from_pretrained('roberta-base')\r\n",
        "tokenizer_xl = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3041e54728bd491092e52eae1c85c706",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c0e602d1b624a79ad5e5cf0b6e5cdc0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c917799c7124a63b709de88fd7235f1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-sGXtv5_VLT"
      },
      "source": [
        "def get_ids_and_mask(tokenizer, x, model):\r\n",
        "    input_ids = []\r\n",
        "    input_mask = []\r\n",
        "    token_type_ids = []\r\n",
        "    if model == \"xl\":\r\n",
        "        x = [k + \" [SEP] [CLS]\" for k in x]\r\n",
        "    for k in range(len(x)):\r\n",
        "        vec = tokenizer.encode_plus(x[k], add_special_tokens=True, max_length=32, pad_to_max_length=True, return_token_type_ids=True)\r\n",
        "        input_ids.append(vec['input_ids'])\r\n",
        "        input_mask.append(vec['attention_mask'])\r\n",
        "        token_type_ids.append(vec['token_type_ids'])\r\n",
        "\r\n",
        "    print(tokenizer.decode(input_ids[0]))\r\n",
        "    return input_ids, input_mask, token_type_ids\r\n",
        "\r\n",
        "def dataloader(sentence, mask, token_type_ids, label, d_type, batch_size=32):\r\n",
        "    loader = None\r\n",
        "    sentence = torch.tensor(sentence)\r\n",
        "    mask = torch.tensor(mask)\r\n",
        "    token_type_ids = torch.tensor(token_type_ids)\r\n",
        "    label = torch.tensor(label)\r\n",
        "\r\n",
        "    print(\"data shape\", sentence.shape)\r\n",
        "    print(\"label shape\", label.shape)\r\n",
        "    print(\"mask shape\", mask.shape)\r\n",
        "    print(\"tok type id shape\", token_type_ids.shape)\r\n",
        "\r\n",
        "    # batch_size = 32\r\n",
        "\r\n",
        "    if d_type == \"train\":\r\n",
        "        train = TensorDataset(sentence, mask, token_type_ids, label)\r\n",
        "        sampler = RandomSampler(train)\r\n",
        "        loader = DataLoader(train, batch_size=batch_size, sampler=sampler)\r\n",
        "\r\n",
        "    else:\r\n",
        "        test = TensorDataset(sentence, mask, token_type_ids, label)\r\n",
        "        samp = SequentialSampler(test)\r\n",
        "        loader = DataLoader(test, batch_size=batch_size, sampler=samp)\r\n",
        "\r\n",
        "    return loader\r\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJPe6jsnoil_"
      },
      "source": [
        "###############   TRAINING OF MODEL  ##################\r\n",
        "\r\n",
        "def train_model(model, optim, epochs, loader):\r\n",
        "    loss_arr = []\r\n",
        "    for ep in range(epochs):\r\n",
        "        model.train()\r\n",
        "        epoch_loss = 0\r\n",
        "\r\n",
        "        for i, data in enumerate(loader):\r\n",
        "            batch_data = data[0].to(device).long()\r\n",
        "            batch_mask = data[1].to(device).long()\r\n",
        "            # batch_token_type_ids = data[2].to(device).long()\r\n",
        "            batch_labels = data[3].to(device).long()\r\n",
        "\r\n",
        "            model.zero_grad()\r\n",
        "            pred = model(batch_data,\r\n",
        "                                token_type_ids=None,\r\n",
        "                                attention_mask=batch_mask,\r\n",
        "                                labels=batch_labels)\r\n",
        "            \r\n",
        "            # As we call the model with labels, it returns the loss in a tuple\r\n",
        "            loss = pred[0]\r\n",
        "            epoch_loss += loss.item()\r\n",
        "            loss.backward()  # Backprpagation\r\n",
        "\r\n",
        "            # Clip Gradient norm to mitigate exploding of gradients\r\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "            optim.step()\r\n",
        "\r\n",
        "        epoch_loss /= len(loader)\r\n",
        "        print(\"train loss after %d epochs is %f \" %(ep+1, epoch_loss))\r\n",
        "        loss_arr.append(epoch_loss)\r\n",
        "\r\n",
        "    return loss_arr\r\n",
        "\r\n",
        "######   Validate RoBerta Model   #######\r\n",
        "\r\n",
        "def validate_model(model, loader):\r\n",
        "    model.eval()\r\n",
        "    pred_list = []\r\n",
        "    true_labels = []\r\n",
        "    test_acc = 0\r\n",
        "    for batch in loader:\r\n",
        "        batch = tuple(t for t in batch)\r\n",
        "        batch_data, batch_mask, batch_token_type ,batch_labels = batch\r\n",
        "        \r\n",
        "        batch_data = batch_data.to(device)\r\n",
        "        batch_mask = batch_mask.to(device)\r\n",
        "        \r\n",
        "        with torch.no_grad():\r\n",
        "            preds = model(batch_data,\r\n",
        "                                    token_type_ids=None,\r\n",
        "                                    attention_mask=batch_mask,\r\n",
        "                                    labels=batch_labels.to(device))\r\n",
        "\r\n",
        "        logits = preds[1]\r\n",
        "        \r\n",
        "        logits = logits.detach().cpu()\r\n",
        "        targets = batch_labels.to('cpu')\r\n",
        "        \r\n",
        "        pred_list.append(logits.numpy())\r\n",
        "        true_labels.append(targets.numpy())\r\n",
        "        \r\n",
        "        acc = compute_accuracy(logits, targets)\r\n",
        "        test_acc += acc\r\n",
        "        # steps += 1\r\n",
        "\r\n",
        "    print(\"final test set accuracy is \", (test_acc / 1043))\r\n",
        "    return pred_list, true_labels\r\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpTKg1S5wQq9"
      },
      "source": [
        "####### Evaluating the GLUE score(Mathew's Correlation for CoLA)  ########\r\n",
        "\r\n",
        "def mathews_corr(pred_list, true_labels):\r\n",
        "    matthews_set = []\r\n",
        "    for i in range(len(true_labels)):\r\n",
        "        pred_labels_i = np.argmax(pred_list[i], axis=1).flatten()\r\n",
        "    \r\n",
        "        # Calculates and stores the coef for this batch.  \r\n",
        "        matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \r\n",
        "        matthews_set.append(matthews)\r\n",
        "\r\n",
        "    flat_predictions = [item for sublist in pred_list for item in sublist]\r\n",
        "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\r\n",
        "\r\n",
        "    # Combine the correct labels for each batch into a single list.\r\n",
        "    flat_true_labels = [item for sublist in true_labels for item in sublist]\r\n",
        "    # Calculate Mathew's correlation\r\n",
        "    mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\r\n",
        "\r\n",
        "    print('MCC: %.3f' % mcc)\r\n",
        "\r\n",
        "def compute_accuracy(preds, targets):\r\n",
        "    return (torch.argmax(preds, dim=1) == targets).float().sum().item()\r\n",
        "    "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p5xEJWrokrF"
      },
      "source": [
        "TILL ABOVE RUN FOR ALL THE 3 APPROACHES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_KrSNtfpmFZ"
      },
      "source": [
        "RUN BELOW CELLS FOR APPROACH 1\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2BGRAnw_bgu",
        "outputId": "4bde68ce-e6e8-4921-fb0e-138908209835"
      },
      "source": [
        "####  APPROACH 1 --> prepare dataloader for Roberta and XL Net   #####\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_train, model=\"bert\")\r\n",
        "train_dataloader = dataloader(ids, masks, tok_type_ids, y_train, \"train\")\r\n",
        "print()\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_val, model=\"bert\")\r\n",
        "val_dataloader = dataloader(ids, masks, tok_type_ids, y_val, \"val\")\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_xl, x=x_train, model=\"xl\")\r\n",
        "train_dataloader_xl = dataloader(ids, masks, tok_type_ids, y_train, \"train\")\r\n",
        "print()\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_xl, x=x_val, model=\"xl\")\r\n",
        "val_dataloader_xl = dataloader(ids, masks, tok_type_ids, y_val, \"val\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<s>Our friends won't buy this analysis, let alone the next one we propose.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "data shape torch.Size([8551, 32])\n",
            "label shape torch.Size([8551])\n",
            "mask shape torch.Size([8551, 32])\n",
            "tok type id shape torch.Size([8551, 32])\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<s>The sailors rode the breeze clear of the rocks.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "data shape torch.Size([1043, 32])\n",
            "label shape torch.Size([1043])\n",
            "mask shape torch.Size([1043, 32])\n",
            "tok type id shape torch.Size([1043, 32])\n",
            "<pad><pad><pad><pad><pad> our friends won't buy this analysis, let alone the next one we propose. [sep] [cls]<sep><cls>\n",
            "data shape torch.Size([8551, 32])\n",
            "label shape torch.Size([8551])\n",
            "mask shape torch.Size([8551, 32])\n",
            "tok type id shape torch.Size([8551, 32])\n",
            "\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> the sailors rode the breeze clear of the rocks. [sep] [cls]<sep><cls>\n",
            "data shape torch.Size([1043, 32])\n",
            "label shape torch.Size([1043])\n",
            "mask shape torch.Size([1043, 32])\n",
            "tok type id shape torch.Size([1043, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds2m30qs_mNm",
        "outputId": "20d40f40-232e-4cc4-8fb8-76164e884058"
      },
      "source": [
        "######   initialize both models   ######\r\n",
        "\r\n",
        "bert_model = RobertaForSequenceClassification.from_pretrained(\r\n",
        "    \"roberta-base\",\r\n",
        "    num_labels=2,\r\n",
        "    output_attentions = False,\r\n",
        "    output_hidden_states = False,\r\n",
        ")\r\n",
        "\r\n",
        "bert_model = bert_model.to(device)\r\n",
        "\r\n",
        "xlnet = XLNetForSequenceClassification.from_pretrained(\r\n",
        "    \"xlnet-base-cased\",\r\n",
        "    num_labels=2,\r\n",
        ")\r\n",
        "xlnet = xlnet.to(device)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pw659LP_qzc"
      },
      "source": [
        "optimizer = AdamW(bert_model.parameters(), lr=4e-5, eps=1e-8)\r\n",
        "optimizer_xl = AdamW(xlnet.parameters(), lr=4e-5, eps=1e-8)\r\n",
        "num_epochs = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h15waXih_yjU",
        "outputId": "86c2597c-7b71-4c8b-fa36-768d2f615f9c"
      },
      "source": [
        "#####   APPROACH 1  --> training both models separately   #####\r\n",
        "\r\n",
        "_  = train_model(bert_model, optimizer, epochs=4, loader=train_dataloader)\r\n",
        "print()\r\n",
        "_ = train_model(xlnet, optimizer_xl, epochs=4, loader=train_dataloader_xl)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train loss after 1 epochs is 0.193445 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_C5KWz9j1ta"
      },
      "source": [
        "####### Evaluating the GLUE score(Mathew's Correlation for CoLA)  ########\r\n",
        "\r\n",
        "def mathews_corr(pred_list, true_labels):\r\n",
        "    matthews_set = []\r\n",
        "    for i in range(len(true_labels)):\r\n",
        "        pred_labels_i = np.argmax(pred_list[i], axis=1).flatten()\r\n",
        "    \r\n",
        "        # Calculates and stores the coef for this batch.  \r\n",
        "        matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \r\n",
        "        matthews_set.append(matthews)\r\n",
        "\r\n",
        "    flat_predictions = [item for sublist in pred_list for item in sublist]\r\n",
        "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\r\n",
        "\r\n",
        "    # Combine the correct labels for each batch into a single list.\r\n",
        "    flat_true_labels = [item for sublist in true_labels for item in sublist]\r\n",
        "    # Calculate Mathew's correlation\r\n",
        "    mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\r\n",
        "\r\n",
        "    print('MCC: %.3f' % mcc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HChi9AkK_0-Q",
        "outputId": "3748132e-9f3b-4fad-c5ed-40b5c110aa88"
      },
      "source": [
        "####   Individual results  ####\r\n",
        "\r\n",
        "print(\"acc using XL NET\")\r\n",
        "pred_list_1, true_labels_1 = validate_model(xlnet, val_dataloader_xl)\r\n",
        "print(\"acc using Roberta\")\r\n",
        "pred_list_2, true_labels_2 = validate_model(bert_model, val_dataloader)\r\n",
        "mathews_corr(pred_list_1, true_labels_1)\r\n",
        "mathews_corr(pred_list_2, true_labels_2)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc using XL NET\n",
            "final test set accuracy is  0.7976989453499521\n",
            "acc using Roberta\n",
            "final test set accuracy is  0.822627037392138\n",
            "MCC: 0.498\n",
            "MCC: 0.566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sEaKlRfkZdZ",
        "outputId": "d1e8f248-0d92-478e-8e62-d2668897e941"
      },
      "source": [
        "######   combine models  --> APPROACH 1    ######\r\n",
        "\r\n",
        "def combine(bert, xlnet, loader_bert, loader_xl):\r\n",
        "\r\n",
        "    bert.eval()\r\n",
        "    xlnet.eval()\r\n",
        "    test_acc = 0\r\n",
        "\r\n",
        "    for batch, batch_xl in zip(loader_bert, loader_xl):\r\n",
        "        batch = tuple(t for t in batch)\r\n",
        "        batch_data, batch_mask, batch_token_type ,batch_labels = batch\r\n",
        "        \r\n",
        "        batch_data = batch_data.to(device)\r\n",
        "        batch_mask = batch_mask.to(device)\r\n",
        "\r\n",
        "        batch_xl = tuple(t for t in batch_xl)\r\n",
        "        batch_data_xl, batch_mask_xl, batch_token_type_xl ,batch_labels_xl = batch_xl\r\n",
        "        \r\n",
        "        batch_data_xl = batch_data_xl.to(device)\r\n",
        "        batch_mask_xl = batch_mask_xl.to(device)\r\n",
        "        \r\n",
        "        with torch.no_grad():\r\n",
        "            preds1 = bert(batch_data, token_type_ids=None, attention_mask=batch_mask, labels=batch_labels.to(device))\r\n",
        "            preds2 = xlnet(batch_data_xl, token_type_ids=None, attention_mask=batch_mask_xl, labels=batch_labels_xl.to(device))\r\n",
        "            \r\n",
        "        logits1 = preds1[1].detach().to('cpu')\r\n",
        "        logits2 = preds2[1].detach().to('cpu')\r\n",
        "        targets = batch_labels.to('cpu')\r\n",
        "        # print(logits1.shape)\r\n",
        "\r\n",
        "        logits = torch.zeros((logits1.shape))\r\n",
        "        for k in range(len(logits1)):\r\n",
        "            if torch.argmax(logits1[k]) >= torch.argmax(logits2[k]):\r\n",
        "                logits[k] = logits1[k]\r\n",
        "            else:\r\n",
        "                logits[k] = logits2[k]\r\n",
        "\r\n",
        "        acc = compute_accuracy(logits, targets)\r\n",
        "        test_acc += acc\r\n",
        "\r\n",
        "    print(\"final test set accuracy using approach 1 is \", (test_acc / 1043))\r\n",
        "\r\n",
        "combine(bert_model, xlnet, val_dataloader, val_dataloader_xl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final test set accuracy using approach 1 is  0.8015340364333653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM9mWkszoy7T"
      },
      "source": [
        "RUN BELOW CELLS FOR APPROACH 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SuzUj2Cm6xf"
      },
      "source": [
        "######   APPROACH 2    ########\r\n",
        "\r\n",
        "class ensemble(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(ensemble, self).__init__()\r\n",
        "        self.bert = transformers.RobertaModel.from_pretrained('roberta-base')\r\n",
        "        self.xlnet = transformers.XLNetModel.from_pretrained('xlnet-base-cased')\r\n",
        "        self.fc = nn.Linear(768*2, 2)\r\n",
        "        self.dropout = nn.Dropout(p=0.3)\r\n",
        "        self.softmax = nn.Softmax(dim=1)\r\n",
        "\r\n",
        "    def forward(self, data_bert, mask_bert, data_xl, mask_xl):\r\n",
        "        \r\n",
        "        out = self.bert(data_bert, attention_mask=mask_bert, token_type_ids=None)\r\n",
        "        emb1 = out[0]\r\n",
        "        emb1 = torch.mean(emb1, dim=1)\r\n",
        "        # print(\"emb1\", emb1.shape)\r\n",
        "        \r\n",
        "        emb2 = self.xlnet(data_xl, attention_mask=mask_xl, token_type_ids=None)\r\n",
        "        emb2 = out[0]\r\n",
        "        emb2 = torch.mean(emb2, dim=1)\r\n",
        "        # print(\"emb2\", emb2.shape)\r\n",
        "\r\n",
        "        emb = torch.cat((emb1, emb2), dim=1)\r\n",
        "\r\n",
        "        return self.softmax(self.fc(emb))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X4PGZmwrcgI"
      },
      "source": [
        "def train_ensemble(model, loader, optim):\r\n",
        "    loss_arr = []\r\n",
        "    for ep in range(4):\r\n",
        "        epoch_loss = 0\r\n",
        "        for i, data in enumerate(loader):\r\n",
        "            model.train()\r\n",
        "            batch_data = data[0].to(device).long()\r\n",
        "            batch_mask = data[1].to(device).long()\r\n",
        "            batch_labels = data[6].to(device).long()\r\n",
        "\r\n",
        "            batch_data_xl = data[3].to(device).long()\r\n",
        "            batch_mask_xl = data[4].to(device).long()\r\n",
        "\r\n",
        "            pred = model(batch_data, batch_mask, batch_data_xl, batch_mask_xl)\r\n",
        "            optim.zero_grad()\r\n",
        "            \r\n",
        "            loss = F.cross_entropy(pred, batch_labels)\r\n",
        "            epoch_loss += loss.item()\r\n",
        "            loss.backward()\r\n",
        "\r\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "            optim.step()\r\n",
        "\r\n",
        "        epoch_loss /= len(loader)\r\n",
        "        print(\"train loss after %d epochs is %f \" %(ep+1, epoch_loss))\r\n",
        "        loss_arr.append(epoch_loss)\r\n",
        "    return loss_arr\r\n",
        "\r\n",
        "\r\n",
        "def validate_ensemble(model, loader):\r\n",
        "    model.eval()\r\n",
        "    pred_list = []\r\n",
        "    true_labels = []\r\n",
        "    test_acc = 0\r\n",
        "    for batch in loader:\r\n",
        "        batch = tuple(t for t in batch)\r\n",
        "        batch_data, batch_mask, z1 , batch_data_xl, batch_mask_xl, z2, batch_labels = batch\r\n",
        "        \r\n",
        "        batch_data = batch_data.to(device)\r\n",
        "        batch_mask = batch_mask.to(device)\r\n",
        "        batch_data_xl = batch_data_xl.to(device)\r\n",
        "        batch_mask_xl = batch_mask_xl.to(device)\r\n",
        "        \r\n",
        "        with torch.no_grad():\r\n",
        "            logits = model(batch_data, batch_mask, batch_data_xl, batch_mask_xl)\r\n",
        "\r\n",
        "        # logits = preds[1]\r\n",
        "        \r\n",
        "        logits = logits.detach().cpu()\r\n",
        "        targets = batch_labels.to('cpu')\r\n",
        "        \r\n",
        "        pred_list.append(logits.numpy())\r\n",
        "        true_labels.append(targets.numpy())\r\n",
        "        \r\n",
        "        acc = compute_accuracy(logits, targets)\r\n",
        "        test_acc += acc\r\n",
        "        # steps += 1\r\n",
        "\r\n",
        "    print(\"final test set accuracy is \", (test_acc / 1043))\r\n",
        "    return pred_list, true_labels\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n42qMl4hgaUV"
      },
      "source": [
        "def dataloader_comb(sent1, sent2, mask1, mask2, id1, id2, label, d_type, batch_size=32):\r\n",
        "    loader = None\r\n",
        "    sent1 = torch.tensor(sent1)\r\n",
        "    sent2 = torch.tensor(sent2)\r\n",
        "    mask1 = torch.tensor(mask1)\r\n",
        "    mask2 = torch.tensor(mask2)\r\n",
        "    id1 = torch.tensor(id1)\r\n",
        "    id2 = torch.tensor(id2)\r\n",
        "    label = torch.tensor(label)\r\n",
        "\r\n",
        "    # print(\"data shape\", sentence.shape)\r\n",
        "    # print(\"label shape\", label.shape)\r\n",
        "    # print(\"mask shape\", mask.shape)\r\n",
        "    # print(\"tok type id shape\", token_type_ids.shape)\r\n",
        "\r\n",
        "    batch_size = 32\r\n",
        "\r\n",
        "    if d_type == \"train\":\r\n",
        "        train = TensorDataset(sent1, mask1, id1, sent2, mask2, id2, label)\r\n",
        "        sampler = RandomSampler(train)\r\n",
        "        loader = DataLoader(train, batch_size=batch_size, sampler=sampler)\r\n",
        "\r\n",
        "    else:\r\n",
        "        test = TensorDataset(sent1, mask1, id1, sent2, mask2, id2, label)\r\n",
        "        samp = SequentialSampler(test)\r\n",
        "        loader = DataLoader(test, batch_size=batch_size, sampler=samp)\r\n",
        "\r\n",
        "    return loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eysPPxbzqQZC",
        "outputId": "ec754cf3-d34e-4cb7-f025-0896220d31fb"
      },
      "source": [
        "\r\n",
        "####   prepare dataloader for combined model   #####\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_train, model=\"bert\")\r\n",
        "ids1, masks1, tok_type_ids1 = get_ids_and_mask(tokenizer=tokenizer_xl, x=x_train, model=\"xl\")\r\n",
        "train_dataloader = dataloader_comb(ids, ids1, masks, masks1, tok_type_ids, tok_type_ids1, y_train, \"train\")\r\n",
        "print()\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_val, model=\"bert\")\r\n",
        "ids1, masks1, tok_type_ids1 = get_ids_and_mask(tokenizer=tokenizer_xl, x=x_val, model=\"xl\")\r\n",
        "val_dataloader = dataloader_comb(ids, ids1, masks, masks1, tok_type_ids, tok_type_ids1, y_val, \"val\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<s>Our friends won't buy this analysis, let alone the next one we propose.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "<pad><pad><pad><pad><pad> our friends won't buy this analysis, let alone the next one we propose. [sep] [cls]<sep><cls>\n",
            "\n",
            "<s>The sailors rode the breeze clear of the rocks.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> the sailors rode the breeze clear of the rocks. [sep] [cls]<sep><cls>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLhsdayYeV0t"
      },
      "source": [
        "ensemble_model = ensemble().to(device)\r\n",
        "optimizer = AdamW(ensemble_model.parameters(), lr=4e-5, eps=1e-10)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfC1TiauEAFP",
        "outputId": "75d95d05-5409-42f7-c034-d194215b9ce7"
      },
      "source": [
        "#####   call train ensemble   ######\r\n",
        "\r\n",
        "_ = train_ensemble(ensemble_model, train_dataloader, optimizer)\r\n",
        "l, m = validate_ensemble(ensemble_model, val_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final test set accuracy is  0.7986577181208053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWIqi2Spo6r4"
      },
      "source": [
        "RUN BELOW CELLS FOR APPROACH 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L28_yMsktYH4",
        "outputId": "a012b11c-9b78-4884-89a1-66db410dd3cb"
      },
      "source": [
        "####  APPROACH 3 starts here   ####\r\n",
        "\r\n",
        "### prepare data to pass to model  ##\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_train, model=\"bert\")\r\n",
        "train_dataloader = dataloader(ids, masks, tok_type_ids, y_train, \"train\", batch_size=32)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<s>Our friends won't buy this analysis, let alone the next one we propose.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "data shape torch.Size([8551, 32])\n",
            "label shape torch.Size([8551])\n",
            "mask shape torch.Size([8551, 32])\n",
            "tok type id shape torch.Size([8551, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjy1riMqn4eU",
        "outputId": "9140b197-1f09-4acd-ffd4-ed4410b83309"
      },
      "source": [
        "bert_model = RobertaForSequenceClassification.from_pretrained(\r\n",
        "    'roberta-base',\r\n",
        "    num_labels=2,\r\n",
        ")\r\n",
        "bert_model = bert_model.to(device)\r\n",
        "optimizer = AdamW(bert_model.parameters(), lr=4e-5, eps=1e-10)\r\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl-3yefeoOz-",
        "outputId": "9e7bcd52-790b-484a-ede8-d960711cc470"
      },
      "source": [
        "###  train model which will score examples  ##\r\n",
        "\r\n",
        "_ = train_model(bert_model, optimizer, epochs=4, loader=train_dataloader)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss after 1 epochs is 0.512399 \n",
            "train loss after 2 epochs is 0.337831 \n",
            "train loss after 3 epochs is 0.245066 \n",
            "train loss after 4 epochs is 0.179049 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHnBdHelqjjs",
        "outputId": "7b17b6ed-0dce-41e6-af20-26b87b615a02"
      },
      "source": [
        "###   prepare train data to get loss om each  ###\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_train, model=\"bert\")\r\n",
        "train_dataloader = dataloader(ids, masks, tok_type_ids, y_train, \"val\", batch_size=1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<s>Our friends won't buy this analysis, let alone the next one we propose.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "data shape torch.Size([8551, 32])\n",
            "label shape torch.Size([8551])\n",
            "mask shape torch.Size([8551, 32])\n",
            "tok type id shape torch.Size([8551, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-e3c0p-uRxO"
      },
      "source": [
        "#####   Approach  3   get loss on all training data   #####\r\n",
        "\r\n",
        "bert_model.eval()\r\n",
        "loss_list = []\r\n",
        "for i, data in enumerate(train_dataloader):\r\n",
        "    batch_data = data[0].to(device).long()\r\n",
        "    batch_mask = data[1].to(device).long()\r\n",
        "    batch_labels = data[3].to(device).long()\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        pred = bert_model(batch_data,\r\n",
        "                                token_type_ids=None,\r\n",
        "                                attention_mask=batch_mask,\r\n",
        "                                labels=batch_labels)\r\n",
        "        \r\n",
        "        \r\n",
        "    loss = pred[0]\r\n",
        "    loss_list.append(loss.item())\r\n",
        "    "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JFwhYQ3zLIn",
        "outputId": "c46baff1-2ccb-4af8-8fe3-f901da5aa18a"
      },
      "source": [
        "########     SORT EXAMPLES BASED ON TOUGHNESS     ######\r\n",
        "\r\n",
        "print(max(loss_list))\r\n",
        "print(min(loss_list))\r\n",
        "\r\n",
        "train_loss = []\r\n",
        "\r\n",
        "for k in range(len(x_train)):\r\n",
        "    train_loss.append((x_train[k], y_train[k], loss_list[k]))\r\n",
        "    # label_loss.append((y_train[k], loss_list[k]))\r\n",
        "\r\n",
        "print(train_loss[-1])\r\n",
        "train_loss = sorted(train_loss, key = lambda x: x[2])\r\n",
        "\r\n",
        "print(train_loss[-1])\r\n",
        "\r\n",
        "x_train_sorted = []\r\n",
        "y_train_sorted = []\r\n",
        "\r\n",
        "for k in train_loss:\r\n",
        "    x_train_sorted.append(k[0])\r\n",
        "    y_train_sorted.append(k[1])\r\n",
        "\r\n",
        "print(x_train_sorted[-1])\r\n",
        "print(len(x_train_sorted), len(y_train_sorted))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.133019924163818\n",
            "0.001861388562247157\n",
            "('What all did you get for Christmas?', 1, 0.005805535241961479)\n",
            "('Dorothy is needing new shoes.', 0, 5.133019924163818)\n",
            "Dorothy is needing new shoes.\n",
            "8551 8551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu2Sem0c2VEf",
        "outputId": "0739acb2-64e0-4d15-cbc1-d4e8eaa66fc7"
      },
      "source": [
        "####   NOW Preapare data in curriculum order from sorted examples  ####\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_train_sorted, model=\"bert\")\r\n",
        "train_dataloader = dataloader(ids, masks, tok_type_ids, y_train_sorted, \"train\", batch_size=32)\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_val, model=\"bert\")\r\n",
        "val_dataloader = dataloader(ids, masks, tok_type_ids, y_val, \"val\", batch_size=32)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<s>I put the book from Edna.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "data shape torch.Size([8551, 32])\n",
            "label shape torch.Size([8551])\n",
            "mask shape torch.Size([8551, 32])\n",
            "tok type id shape torch.Size([8551, 32])\n",
            "<s>The sailors rode the breeze clear of the rocks.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "data shape torch.Size([1043, 32])\n",
            "label shape torch.Size([1043])\n",
            "mask shape torch.Size([1043, 32])\n",
            "tok type id shape torch.Size([1043, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FuOEhlt3isU",
        "outputId": "9f90ee8d-7afc-4a28-8a7a-65ecd1f7f08e"
      },
      "source": [
        "###  model for training in curriculum oreder  ###\r\n",
        "\r\n",
        "new_model = RobertaForSequenceClassification.from_pretrained(\r\n",
        "    \"roberta-base\",\r\n",
        "    num_labels=2,\r\n",
        "    output_attentions = False,\r\n",
        "    output_hidden_states = False,\r\n",
        ")\r\n",
        "\r\n",
        "new_model = new_model.to(device)\r\n",
        "optim = AdamW(new_model.parameters(), lr=4e-5, eps=1e-8)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXHObodV2eLT",
        "outputId": "224c4181-c4cd-40f2-e8eb-30116f0a8500"
      },
      "source": [
        "###  train model  ####\r\n",
        "\r\n",
        "arr = train_model(new_model, optim, 2, train_dataloader)\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_train, model=\"bert\")\r\n",
        "train_dataloader = dataloader(ids, masks, tok_type_ids, y_train, \"train\", batch_size=32)\r\n",
        "arr = train_model(new_model, optim, 2, train_dataloader)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss after 1 epochs is 0.494032 \n",
            "train loss after 2 epochs is 0.323597 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<s>Our friends won't buy this analysis, let alone the next one we propose.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "data shape torch.Size([8551, 32])\n",
            "label shape torch.Size([8551])\n",
            "mask shape torch.Size([8551, 32])\n",
            "tok type id shape torch.Size([8551, 32])\n",
            "train loss after 1 epochs is 0.234240 \n",
            "train loss after 2 epochs is 0.180365 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4dfY_Ev3WE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07871ea-2bdb-42e7-9cd1-cfeb430e629f"
      },
      "source": [
        "_,_ = validate_model(new_model, val_dataloader)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final test set accuracy is  0.8053691275167785\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}