{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SST_experiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "159abff7412140beb47ff449e4ae71fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_969f058c946146d79e306e9be8d229c9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_762b31bb42ac44a7ab953ba9148c9635",
              "IPY_MODEL_d72ac2a8ab484acbb39556435244887b"
            ]
          }
        },
        "969f058c946146d79e306e9be8d229c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "762b31bb42ac44a7ab953ba9148c9635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e7e9c5bcd53a49cf8d191a86a9bd7e62",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 760,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 760,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ddc47bf67ca14ec38a97a9840fc3e6e7"
          }
        },
        "d72ac2a8ab484acbb39556435244887b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4cfb3246b82e4f70b41e3b49d503043d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 760/760 [00:00&lt;00:00, 2.78kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_beeafb82a5de455a8e807b73ad61688e"
          }
        },
        "e7e9c5bcd53a49cf8d191a86a9bd7e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ddc47bf67ca14ec38a97a9840fc3e6e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4cfb3246b82e4f70b41e3b49d503043d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "beeafb82a5de455a8e807b73ad61688e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3480c80c443a40cebe32eded3649031f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9d52e2b2dca042ac8520d3c5aaa9204c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_05d2217517e7433a9adfdd723fa4e1e5",
              "IPY_MODEL_a9c767e13714499a92526473fb3a5b49"
            ]
          }
        },
        "9d52e2b2dca042ac8520d3c5aaa9204c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05d2217517e7433a9adfdd723fa4e1e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f58133c301af48d5bb22503791ec8335",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 467042463,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 467042463,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e204c606cefc4455be60efbf89fc5457"
          }
        },
        "a9c767e13714499a92526473fb3a5b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_630b376ef8ec4d829f1fa977ccf7f2d1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 467M/467M [00:33&lt;00:00, 14.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78d340ccd0704f56b86cee0c349017a0"
          }
        },
        "f58133c301af48d5bb22503791ec8335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e204c606cefc4455be60efbf89fc5457": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "630b376ef8ec4d829f1fa977ccf7f2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78d340ccd0704f56b86cee0c349017a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "81bac6263d6246fda1cb1dbfe520edcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6facae3502824951ab93ea25abc301a3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9fc373aa385a4b4aa50bb723fcbbb421",
              "IPY_MODEL_a6c75a7af6cf4148aa9be3435afcf92c"
            ]
          }
        },
        "6facae3502824951ab93ea25abc301a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9fc373aa385a4b4aa50bb723fcbbb421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fde03229e8344af7a55c7be86a62d7b1",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 760,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 760,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09ba29dd65f8491aa063723f05c81e8c"
          }
        },
        "a6c75a7af6cf4148aa9be3435afcf92c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9cd4861eda28429fbaf2e69e7180730f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 760/760 [00:00&lt;00:00, 4.46kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cbbbac70c7ba40bf8f071bfc2db6897c"
          }
        },
        "fde03229e8344af7a55c7be86a62d7b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09ba29dd65f8491aa063723f05c81e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9cd4861eda28429fbaf2e69e7180730f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cbbbac70c7ba40bf8f071bfc2db6897c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f96be508687f4409904c786f7e79af5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6c4be4a87ec74161950461576375934b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ec2a97652034440cbfa70ff1c32f8318",
              "IPY_MODEL_c7ff503e257949ddbc0d3d4d025fda89"
            ]
          }
        },
        "6c4be4a87ec74161950461576375934b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec2a97652034440cbfa70ff1c32f8318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ac1cc67071384fdda3ab25bccab8fa9a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 467042463,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 467042463,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7183c12cf5294ba2a98934971b16f56a"
          }
        },
        "c7ff503e257949ddbc0d3d4d025fda89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b852aabb365e40cc9a76794bd549d6c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 467M/467M [00:06&lt;00:00, 70.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_043beff6587f45d4868da33a22d7f192"
          }
        },
        "ac1cc67071384fdda3ab25bccab8fa9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7183c12cf5294ba2a98934971b16f56a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b852aabb365e40cc9a76794bd549d6c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "043beff6587f45d4868da33a22d7f192": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZAA9CUdlU7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44562d46-a9b2-49b0-ce57-d5198f3c3ced"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import string\r\n",
        "from statistics import mean\r\n",
        " \r\n",
        "import copy\r\n",
        "import os\r\n",
        "import re\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim\r\n",
        "import nltk\r\n",
        " \r\n",
        "from nltk.corpus import stopwords\r\n",
        "import spacy\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "import torch.nn.functional as F\r\n",
        " \r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        " \r\n",
        "!pip install transformers\r\n",
        "!pip install pytorch-transformers\r\n",
        " \r\n",
        "import transformers\r\n",
        "from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\r\n",
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\r\n",
        "from transformers import RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\r\n",
        "from transformers import AdamW\r\n",
        " \r\n",
        "from tqdm import tqdm, trange\r\n",
        " \r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.16.57)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.1.95)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.0.43)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.19.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.16.0)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.57 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.19.57)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.3.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.57->boto3->pytorch-transformers) (2.8.1)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyydRF7_l2Fs",
        "outputId": "4164f497-433c-4071-830c-c42505ef48cc"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK3dn-qPl6Zr"
      },
      "source": [
        "def clean_data(x):\r\n",
        "    puncts = []\r\n",
        "    # puncts = puncts.replace(\"-\", \"\")\r\n",
        "    # stop_words = set(stopwords.words('english'))\r\n",
        "    sentences = []\r\n",
        "    # x = [''.join(c.lower() for c in s.replace(\"-\", \" \") if c not in puncts) for s in x]\r\n",
        "    x = [''.join(c.lower() for c in s if c not in puncts) for s in x]\r\n",
        "    # x = [''.join(c for c in str(s)[1:-4].replace(\"-\", \" \") if c not in puncts) for s in x]\r\n",
        "    for sent in x:\r\n",
        "        # text_no_nums = re.sub(r'\\d+', '', sent)\r\n",
        "        # text_no_doublespace = re.sub('\\s+', ' ', text_no_nums).strip()\r\n",
        "        sentences.append(sent)\r\n",
        "    return sentences  \r\n",
        "\r\n",
        "def preprocess_targets(y):\r\n",
        "    # dic = {'1.0': 0, '2.0': 1, '3.0': 2, '4.0': 3, '5.0': 4}\r\n",
        "    # dic_SST2 = {0: \"POS\", 1: \"NEG\"}\r\n",
        "    dic = {'NEG': 1, 'POS': 0}\r\n",
        "    for t in range(len(y)):\r\n",
        "        y[t] = dic[y[t]]\r\n",
        "    y = torch.tensor(y, dtype=torch.long)   \r\n",
        "    return y  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4nRfxWSl-5C",
        "outputId": "86ec2ce2-e53b-4d72-d523-9653210a02fc"
      },
      "source": [
        "# path = 'drive/My Drive/third_sem/DL_NLP/Assignment3/digital_music_5.'\r\n",
        "# path = 'drive/My Drive/third_sem/DL_NLP/Assignment2/SST-2.'\r\n",
        "path = \"/content/drive/MyDrive/third_sem/DL_NLP/Project/dataset/SST-2/\"\r\n",
        "\r\n",
        "train_path = path + 'train.tsv'\r\n",
        "val_path = path + 'dev.tsv'\r\n",
        "test_path = path + 'test.tsv'\r\n",
        "\r\n",
        "df = pd.read_csv(train_path, sep='\\t')\r\n",
        "print(df)\r\n",
        "x_train = df[\"sentence\"].values\r\n",
        "y_train = df[\"label\"].values\r\n",
        "print(x_train[-1])\r\n",
        "print(y_train[-1])\r\n",
        "\r\n",
        "df = pd.read_csv(val_path, sep='\\t')\r\n",
        "x_val = df[\"sentence\"].values\r\n",
        "y_val = df[\"label\"].values\r\n",
        "\r\n",
        "# x_train = clean_data(x_train)\r\n",
        "print(x_train[:5])\r\n",
        "\r\n",
        "# x_val = clean_data(x_val)\r\n",
        "print(\"hereee\",x_val[:2])\r\n",
        "# # x_train = [x + \" [SEP] [CLS]\" for x in x_train]\r\n",
        "\r\n",
        "# df = pd.read_csv(test_path, sep='\\t')\r\n",
        "# x_test = df[\"sentence\"].values\r\n",
        "# print(df)\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                sentence  label\n",
            "0           hide new secretions from the parental units       0\n",
            "1                   contains no wit , only labored gags       0\n",
            "2      that loves its characters and communicates som...      1\n",
            "3      remains utterly satisfied to remain the same t...      0\n",
            "4      on the worst revenge-of-the-nerds clichÃ©s the ...      0\n",
            "...                                                  ...    ...\n",
            "67344                               a delightful comedy       1\n",
            "67345                   anguish , anger and frustration       0\n",
            "67346  at achieving the modest , crowd-pleasing goals...      1\n",
            "67347                                  a patient viewer       1\n",
            "67348  this new jangle of noise , mayhem and stupidit...      0\n",
            "\n",
            "[67349 rows x 2 columns]\n",
            "this new jangle of noise , mayhem and stupidity must be a serious contender for the title . \n",
            "0\n",
            "['hide new secretions from the parental units '\n",
            " 'contains no wit , only labored gags '\n",
            " 'that loves its characters and communicates something rather beautiful about human nature '\n",
            " 'remains utterly satisfied to remain the same throughout '\n",
            " 'on the worst revenge-of-the-nerds clichÃ©s the filmmakers could dredge up ']\n",
            "hereee [\"it 's a charming and often affecting journey . \"\n",
            " 'unflinchingly bleak and desperate ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49S5b6YKnqZD"
      },
      "source": [
        "tokenizer_bert = RobertaTokenizer.from_pretrained('roberta-base')\r\n",
        "tokenizer_xl = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql-lnX1GoK3j"
      },
      "source": [
        "def get_ids_and_mask(tokenizer, x, model):\r\n",
        "    input_ids = []\r\n",
        "    input_mask = []\r\n",
        "    token_type_ids = []\r\n",
        "    if model == \"xl\":\r\n",
        "        x = [k + \" [SEP] [CLS]\" for k in x]\r\n",
        "    for k in range(len(x)):\r\n",
        "        vec = tokenizer.encode_plus(x[k], add_special_tokens=True, max_length=40, pad_to_max_length=True, return_token_type_ids=True)\r\n",
        "        input_ids.append(vec['input_ids'])\r\n",
        "        input_mask.append(vec['attention_mask'])\r\n",
        "        token_type_ids.append(vec['token_type_ids'])\r\n",
        "\r\n",
        "    print(tokenizer.decode(input_ids[0]))\r\n",
        "    return input_ids, input_mask, token_type_ids\r\n",
        "\r\n",
        "def dataloader(sentence, mask, token_type_ids, label, d_type, batch_size=32):\r\n",
        "    loader = None\r\n",
        "    sentence = torch.tensor(sentence)\r\n",
        "    mask = torch.tensor(mask)\r\n",
        "    token_type_ids = torch.tensor(token_type_ids)\r\n",
        "    label = torch.tensor(label)\r\n",
        "\r\n",
        "    print(\"data shape\", sentence.shape)\r\n",
        "    print(\"label shape\", label.shape)\r\n",
        "    print(\"mask shape\", mask.shape)\r\n",
        "    print(\"tok type id shape\", token_type_ids.shape)\r\n",
        "\r\n",
        "    # batch_size = 32\r\n",
        "\r\n",
        "    if d_type == \"train\":\r\n",
        "        train = TensorDataset(sentence, mask, token_type_ids, label)\r\n",
        "        sampler = RandomSampler(train)\r\n",
        "        loader = DataLoader(train, batch_size=batch_size, sampler=sampler)\r\n",
        "\r\n",
        "    else:\r\n",
        "        test = TensorDataset(sentence, mask, token_type_ids, label)\r\n",
        "        samp = SequentialSampler(test)\r\n",
        "        loader = DataLoader(test, batch_size=batch_size, sampler=samp)\r\n",
        "\r\n",
        "    return loader\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ind42KRb110K"
      },
      "source": [
        "###############   TRAINING OF MODEL  ##################\r\n",
        "\r\n",
        "def train_model(model, optim, epochs, loader):\r\n",
        "    loss_arr = []\r\n",
        "    for ep in range(epochs):\r\n",
        "        model.train()\r\n",
        "        epoch_loss = 0\r\n",
        "\r\n",
        "        for i, data in enumerate(loader):\r\n",
        "            batch_data = data[0].to(device).long()\r\n",
        "            batch_mask = data[1].to(device).long()\r\n",
        "            # batch_token_type_ids = data[2].to(device).long()\r\n",
        "            batch_labels = data[3].to(device).long()\r\n",
        "\r\n",
        "            model.zero_grad()\r\n",
        "            pred = model(batch_data,\r\n",
        "                                token_type_ids=None,\r\n",
        "                                attention_mask=batch_mask,\r\n",
        "                                labels=batch_labels)\r\n",
        "            \r\n",
        "            # As we call the model with labels, it returns the loss in a tuple\r\n",
        "            loss = pred[0]\r\n",
        "            epoch_loss += loss.item()\r\n",
        "            loss.backward()  # Backprpagation\r\n",
        "\r\n",
        "            # Clip Gradient norm to mitigate exploding of gradients\r\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "            optim.step()\r\n",
        "\r\n",
        "        epoch_loss /= len(loader)\r\n",
        "        print(\"train loss after %d epochs is %f \" %(ep+1, epoch_loss))\r\n",
        "        loss_arr.append(epoch_loss)\r\n",
        "\r\n",
        "    return loss_arr\r\n",
        "\r\n",
        "######   Validate Model   #######\r\n",
        "\r\n",
        "def validate_model(model, loader):\r\n",
        "    model.eval()\r\n",
        "    test_acc = 0\r\n",
        "    for batch in loader:\r\n",
        "        batch = tuple(t for t in batch)\r\n",
        "        batch_data, batch_mask, batch_token_type ,batch_labels = batch\r\n",
        "        \r\n",
        "        batch_data = batch_data.to(device)\r\n",
        "        batch_mask = batch_mask.to(device)\r\n",
        "        \r\n",
        "        with torch.no_grad():\r\n",
        "            preds = model(batch_data.to(device),\r\n",
        "                                    token_type_ids=None,\r\n",
        "                                    attention_mask=batch_mask)\r\n",
        "\r\n",
        "        logits = preds[0]\r\n",
        "        \r\n",
        "        logits = logits.detach().cpu()\r\n",
        "        targets = batch_labels.to('cpu')\r\n",
        "        \r\n",
        "        acc = compute_accuracy(logits, targets)\r\n",
        "        test_acc += acc\r\n",
        "        # steps += 1\r\n",
        "\r\n",
        "    print(\"final test set accuracy is \", (test_acc / 872))\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUTokDsf2iU-"
      },
      "source": [
        "def compute_accuracy(preds, targets):\r\n",
        "    return (torch.argmax(preds, dim=1) == targets).float().sum().item()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WFwlJk22mLR"
      },
      "source": [
        "RUN ABOVE CELLS FOR ALL APPROACHES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLpCegNe2q1k"
      },
      "source": [
        "RUN BELOW CELLS FOR APPROACH 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USuVJTj2rDUc",
        "outputId": "7e03c029-1005-4eff-a8a0-85d63488fbd1"
      },
      "source": [
        "#####   APPROACH 1  ######\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_train, model=\"bert\")\r\n",
        "train_dataloader = dataloader(ids, masks, tok_type_ids, y_train, \"train\")\r\n",
        "print()\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_val, model=\"bert\")\r\n",
        "val_dataloader = dataloader(ids, masks, tok_type_ids, y_val, \"val\")\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<s>hide new secretions from the parental units </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "data shape torch.Size([67349, 40])\n",
            "label shape torch.Size([67349])\n",
            "mask shape torch.Size([67349, 40])\n",
            "tok type id shape torch.Size([67349, 40])\n",
            "\n",
            "<s>it's a charming and often affecting journey. </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "data shape torch.Size([872, 40])\n",
            "label shape torch.Size([872])\n",
            "mask shape torch.Size([872, 40])\n",
            "tok type id shape torch.Size([872, 40])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsHh5awCsFX2",
        "outputId": "294c3f6e-20df-497b-8751-86d7d6f648f6"
      },
      "source": [
        "bert_model = RobertaForSequenceClassification.from_pretrained(\r\n",
        "    \"roberta-base\",\r\n",
        "    num_labels=2,\r\n",
        "    output_attentions = False,\r\n",
        "    output_hidden_states = False,\r\n",
        ")\r\n",
        "\r\n",
        "bert_model = bert_model.to(device)\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWZ72Z2nvQtt"
      },
      "source": [
        "########  define optimizer, write accuracy fn  ###########\r\n",
        "\r\n",
        "optimizer = AdamW(bert_model.parameters(), lr=4e-5, eps=1e-8)\r\n",
        "num_epochs = 4"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSAYd8XZwqzm"
      },
      "source": [
        "arr = train_bert(bert_model, optimizer, 4, train_dataloader)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwyQDI7sxnzn",
        "outputId": "65ca89db-63d3-492f-f24a-32bf5303950f"
      },
      "source": [
        "validate_model(bert_model, val_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final test set accuracy is  0.9197247706422018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vPea3OL8ZjU",
        "outputId": "ec7325cb-063d-4dd3-ede1-606360fbbfb0"
      },
      "source": [
        "####   APPROACH 1 --> Second model   #####\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_xl, x=x_train, model=\"xl\")\r\n",
        "train_dataloader_xl = dataloader(ids, masks, tok_type_ids, y_train, \"train\")\r\n",
        "print()\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_xl, x=x_val, model=\"xl\")\r\n",
        "val_dataloader_xl = dataloader(ids, masks, tok_type_ids, y_val, \"val\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2137: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> hide new secretions from the parental units [sep] [cls]<sep><cls>\n",
            "data shape torch.Size([67349, 40])\n",
            "label shape torch.Size([67349])\n",
            "mask shape torch.Size([67349, 40])\n",
            "tok type id shape torch.Size([67349, 40])\n",
            "\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> it's a charming and often affecting journey. [sep] [cls]<sep><cls>\n",
            "data shape torch.Size([872, 40])\n",
            "label shape torch.Size([872])\n",
            "mask shape torch.Size([872, 40])\n",
            "tok type id shape torch.Size([872, 40])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218,
          "referenced_widgets": [
            "159abff7412140beb47ff449e4ae71fe",
            "969f058c946146d79e306e9be8d229c9",
            "762b31bb42ac44a7ab953ba9148c9635",
            "d72ac2a8ab484acbb39556435244887b",
            "e7e9c5bcd53a49cf8d191a86a9bd7e62",
            "ddc47bf67ca14ec38a97a9840fc3e6e7",
            "4cfb3246b82e4f70b41e3b49d503043d",
            "beeafb82a5de455a8e807b73ad61688e",
            "3480c80c443a40cebe32eded3649031f",
            "9d52e2b2dca042ac8520d3c5aaa9204c",
            "05d2217517e7433a9adfdd723fa4e1e5",
            "a9c767e13714499a92526473fb3a5b49",
            "f58133c301af48d5bb22503791ec8335",
            "e204c606cefc4455be60efbf89fc5457",
            "630b376ef8ec4d829f1fa977ccf7f2d1",
            "78d340ccd0704f56b86cee0c349017a0"
          ]
        },
        "id": "Q825j-2a9fXR",
        "outputId": "72e326a7-8c7a-4ecb-d0af-d7087c0e6c10"
      },
      "source": [
        "xlnet = XLNetForSequenceClassification.from_pretrained(\r\n",
        "    \"xlnet-base-cased\",\r\n",
        "    num_labels=2,\r\n",
        "    output_attentions = False,\r\n",
        "    output_hidden_states = False,\r\n",
        ")\r\n",
        "\r\n",
        "xlnet = xlnet.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "159abff7412140beb47ff449e4ae71fe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3480c80c443a40cebe32eded3649031f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=467042463.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuupYuHg9WND"
      },
      "source": [
        "optimizer = AdamW(xlnet.parameters(), lr=4e-5, eps=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GBDMAv3-DwF",
        "outputId": "fd0fb399-5953-47e3-c2c7-5fb5c317cfe6"
      },
      "source": [
        "arr = train_model(xlnet, optimizer, 4, train_dataloader_xl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss after 1 epochs is 0.238430 \n",
            "train loss after 2 epochs is 0.155963 \n",
            "train loss after 3 epochs is 0.122752 \n",
            "train loss after 4 epochs is 0.112080 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdwB1tNv-Ncb",
        "outputId": "d8a8407e-fc1d-4417-d2aa-93e28615e0e3"
      },
      "source": [
        "validate_model(xlnet, val_dataloader_xl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final test set accuracy is  0.9048165137614679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaRaU3XOBzs2"
      },
      "source": [
        "######   APPROACH 1 --> combine models    ######\r\n",
        "\r\n",
        "def combine(bert, xlnet, loader_bert, loader_xl):\r\n",
        "\r\n",
        "    bert.eval()\r\n",
        "    xlnet.eval()\r\n",
        "    test_acc = 0\r\n",
        "\r\n",
        "    for batch, batch_xl in zip(loader_bert, loader_xl):\r\n",
        "        batch = tuple(t for t in batch)\r\n",
        "        batch_data, batch_mask, batch_token_type ,batch_labels = batch\r\n",
        "        \r\n",
        "        batch_data = batch_data.to(device)\r\n",
        "        batch_mask = batch_mask.to(device)\r\n",
        "\r\n",
        "        batch_xl = tuple(t for t in batch_xl)\r\n",
        "        batch_data_xl, batch_mask_xl, batch_token_type_xl ,batch_labels_xl = batch_xl\r\n",
        "        \r\n",
        "        batch_data_xl = batch_data_xl.to(device)\r\n",
        "        batch_mask_xl = batch_mask_xl.to(device)\r\n",
        "        \r\n",
        "        with torch.no_grad():\r\n",
        "            preds1 = bert(batch_data, token_type_ids=None, attention_mask=batch_mask, labels=batch_labels.to(device))\r\n",
        "            preds2 = xlnet(batch_data_xl, token_type_ids=None, attention_mask=batch_mask_xl, labels=batch_labels_xl.to(device))\r\n",
        "            \r\n",
        "        logits1 = preds1[1].detach().to('cpu')\r\n",
        "        logits2 = preds2[1].detach().to('cpu')\r\n",
        "        targets = batch_labels.to('cpu')\r\n",
        "\r\n",
        "        logits = torch.zeros((logits1.shape))\r\n",
        "        for k in range(len(logits1)):\r\n",
        "            if torch.argmax(logits1[k]) >= torch.argmax(logits2[k]):\r\n",
        "                logits[k] = logits1[k]\r\n",
        "            else:\r\n",
        "                logits[k] = logits2[k]\r\n",
        "\r\n",
        "        acc = compute_accuracy(logits, targets)\r\n",
        "        test_acc += acc\r\n",
        "\r\n",
        "    print(\"final test set accuracy is \", (test_acc / 872))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsq0TgV7EWSM",
        "outputId": "b0e2c36e-b412-4c87-de5d-a537910600a6"
      },
      "source": [
        "####  APPROACH 1 result   ######\r\n",
        "\r\n",
        "combine(bert_model, xlnet, val_dataloader, val_dataloader_xl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final test set accuracy is  0.9288990825688074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AOeM-ea4XPH"
      },
      "source": [
        "APPROACH 2 starts here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTWkpYwD4WYP"
      },
      "source": [
        "######   APPROACH 2    ########\r\n",
        "\r\n",
        "class ensemble(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(ensemble, self).__init__()\r\n",
        "        self.bert = transformers.RobertaModel.from_pretrained('roberta-base')\r\n",
        "        self.xlnet = transformers.XLNetModel.from_pretrained('xlnet-base-cased')\r\n",
        "        self.fc = nn.Linear(768*2, 2)\r\n",
        "        self.dropout = nn.Dropout(p=0.3)\r\n",
        "        self.softmax = nn.Softmax(dim=1)\r\n",
        "\r\n",
        "    def forward(self, data_bert, mask_bert, data_xl, mask_xl):\r\n",
        "        \r\n",
        "        out = self.bert(data_bert, attention_mask=mask_bert, token_type_ids=None)\r\n",
        "        emb1 = out[0]\r\n",
        "        emb1 = torch.mean(emb1, dim=1)\r\n",
        "        # print(\"emb1\", emb1.shape)\r\n",
        "        \r\n",
        "        emb2 = self.xlnet(data_xl, attention_mask=mask_xl, token_type_ids=None)\r\n",
        "        emb2 = out[0]\r\n",
        "        emb2 = torch.mean(emb2, dim=1)\r\n",
        "        # print(\"emb2\", emb2.shape)\r\n",
        "\r\n",
        "        emb = torch.cat((emb1, emb2), dim=1)\r\n",
        "\r\n",
        "        return self.softmax(self.fc(emb))\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYBQKO2f4fRd"
      },
      "source": [
        "######   Approach 2   train ensemble of roberta and xlnet   #####\r\n",
        "\r\n",
        "def train_ensemble(model, loader, optim):\r\n",
        "    loss_arr = []\r\n",
        "    for ep in range(4):\r\n",
        "        epoch_loss = 0\r\n",
        "        for i, data in enumerate(loader):\r\n",
        "            model.train()\r\n",
        "            batch_data = data[0].to(device).long()\r\n",
        "            batch_mask = data[1].to(device).long()\r\n",
        "            batch_labels = data[6].to(device).long()\r\n",
        "\r\n",
        "            batch_data_xl = data[3].to(device).long()\r\n",
        "            batch_mask_xl = data[4].to(device).long()\r\n",
        "\r\n",
        "            pred = model(batch_data, batch_mask, batch_data_xl, batch_mask_xl)\r\n",
        "            optim.zero_grad()\r\n",
        "            \r\n",
        "            loss = F.cross_entropy(pred, batch_labels)\r\n",
        "            epoch_loss += loss.item()\r\n",
        "            loss.backward()\r\n",
        "\r\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "            optim.step()\r\n",
        "\r\n",
        "        epoch_loss /= len(loader)\r\n",
        "        print(\"train loss after %d epochs is %f \" %(ep+1, epoch_loss))\r\n",
        "        loss_arr.append(epoch_loss)\r\n",
        "    return loss_arr\r\n",
        "\r\n",
        "\r\n",
        "def validate_ensemble(model, loader):\r\n",
        "    model.eval()\r\n",
        "    pred_list = []\r\n",
        "    true_labels = []\r\n",
        "    test_acc = 0\r\n",
        "    for batch in loader:\r\n",
        "        batch = tuple(t for t in batch)\r\n",
        "        batch_data, batch_mask, z1 , batch_data_xl, batch_mask_xl, z2, batch_labels = batch\r\n",
        "        \r\n",
        "        batch_data = batch_data.to(device)\r\n",
        "        batch_mask = batch_mask.to(device)\r\n",
        "        batch_data_xl = batch_data_xl.to(device)\r\n",
        "        batch_mask_xl = batch_mask_xl.to(device)\r\n",
        "        \r\n",
        "        with torch.no_grad():\r\n",
        "            logits = model(batch_data, batch_mask, batch_data_xl, batch_mask_xl)\r\n",
        "\r\n",
        "        # logits = preds[1]\r\n",
        "        \r\n",
        "        logits = logits.detach().cpu()\r\n",
        "        targets = batch_labels.to('cpu')\r\n",
        "        \r\n",
        "        pred_list.append(logits.numpy())\r\n",
        "        true_labels.append(targets.numpy())\r\n",
        "        \r\n",
        "        acc = compute_accuracy(logits, targets)\r\n",
        "        test_acc += acc\r\n",
        "        # steps += 1\r\n",
        "\r\n",
        "    print(\"final test set accuracy is \", (test_acc / 1043))\r\n",
        "    return pred_list, true_labels\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srLkkjLy4pRG"
      },
      "source": [
        "##### combined dataloader for approach 2   ###\r\n",
        "\r\n",
        "def dataloader_comb(sent1, sent2, mask1, mask2, id1, id2, label, d_type, batch_size=32):\r\n",
        "    loader = None\r\n",
        "    sent1 = torch.tensor(sent1)\r\n",
        "    sent2 = torch.tensor(sent2)\r\n",
        "    mask1 = torch.tensor(mask1)\r\n",
        "    mask2 = torch.tensor(mask2)\r\n",
        "    id1 = torch.tensor(id1)\r\n",
        "    id2 = torch.tensor(id2)\r\n",
        "    label = torch.tensor(label)\r\n",
        "\r\n",
        "    # print(\"data shape\", sentence.shape)\r\n",
        "    # print(\"label shape\", label.shape)\r\n",
        "    # print(\"mask shape\", mask.shape)\r\n",
        "    # print(\"tok type id shape\", token_type_ids.shape)\r\n",
        "\r\n",
        "    batch_size = 32\r\n",
        "\r\n",
        "    if d_type == \"train\":\r\n",
        "        train = TensorDataset(sent1, mask1, id1, sent2, mask2, id2, label)\r\n",
        "        sampler = RandomSampler(train)\r\n",
        "        loader = DataLoader(train, batch_size=batch_size, sampler=sampler)\r\n",
        "\r\n",
        "    else:\r\n",
        "        test = TensorDataset(sent1, mask1, id1, sent2, mask2, id2, label)\r\n",
        "        samp = SequentialSampler(test)\r\n",
        "        loader = DataLoader(test, batch_size=batch_size, sampler=samp)\r\n",
        "\r\n",
        "    return loader"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV63tQ2U4y_u",
        "outputId": "e1130709-a004-46ed-984c-c225c9688060"
      },
      "source": [
        "####   prepare dataloader for combined model   #####\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_train, model=\"bert\")\r\n",
        "ids1, masks1, tok_type_ids1 = get_ids_and_mask(tokenizer=tokenizer_xl, x=x_train, model=\"xl\")\r\n",
        "train_dataloader = dataloader_comb(ids, ids1, masks, masks1, tok_type_ids, tok_type_ids1, y_train, \"train\")\r\n",
        "print()\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_val, model=\"bert\")\r\n",
        "ids1, masks1, tok_type_ids1 = get_ids_and_mask(tokenizer=tokenizer_xl, x=x_val, model=\"xl\")\r\n",
        "val_dataloader = dataloader_comb(ids, ids1, masks, masks1, tok_type_ids, tok_type_ids1, y_val, \"val\")\r\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<s>hide new secretions from the parental units </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> hide new secretions from the parental units [sep] [cls]<sep><cls>\n",
            "\n",
            "<s>it's a charming and often affecting journey. </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> it's a charming and often affecting journey. [sep] [cls]<sep><cls>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114,
          "referenced_widgets": [
            "81bac6263d6246fda1cb1dbfe520edcd",
            "6facae3502824951ab93ea25abc301a3",
            "9fc373aa385a4b4aa50bb723fcbbb421",
            "a6c75a7af6cf4148aa9be3435afcf92c",
            "fde03229e8344af7a55c7be86a62d7b1",
            "09ba29dd65f8491aa063723f05c81e8c",
            "9cd4861eda28429fbaf2e69e7180730f",
            "cbbbac70c7ba40bf8f071bfc2db6897c",
            "f96be508687f4409904c786f7e79af5a",
            "6c4be4a87ec74161950461576375934b",
            "ec2a97652034440cbfa70ff1c32f8318",
            "c7ff503e257949ddbc0d3d4d025fda89",
            "ac1cc67071384fdda3ab25bccab8fa9a",
            "7183c12cf5294ba2a98934971b16f56a",
            "b852aabb365e40cc9a76794bd549d6c9",
            "043beff6587f45d4868da33a22d7f192"
          ]
        },
        "id": "h6eaqPMj43w2",
        "outputId": "d08c9e05-0aa6-4222-acbe-16ac0cac98df"
      },
      "source": [
        "ensemble_model = ensemble().to(device)\r\n",
        "optimizer = AdamW(ensemble_model.parameters(), lr=4e-5, eps=1e-10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81bac6263d6246fda1cb1dbfe520edcd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f96be508687f4409904c786f7e79af5a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=467042463.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypzq8fXU5A9I"
      },
      "source": [
        "####  Approach 2 ... train the ensemble   ####\r\n",
        "\r\n",
        "#####   call train ensemble   ######\r\n",
        "\r\n",
        "_ = train_ensemble(ensemble_model, train_dataloader, optimizer)\r\n",
        "validate_ensemble(ensemble_model, val_dataloader)\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jM1Jzf25Rgt"
      },
      "source": [
        "RUN BELOW CELLS FOR APPROACH 3 --> CURRICULUM LEARNING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us6rn0eu5PJv",
        "outputId": "04362531-d720-4725-a622-6b05d6b54ae2"
      },
      "source": [
        "####  APPROACH 3 starts here   ####\r\n",
        "\r\n",
        "### prepare data to pass to model  ##\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_train, model=\"bert\")\r\n",
        "train_dataloader = dataloader(ids, masks, tok_type_ids, y_train, \"train\", batch_size=32)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<s>hide new secretions from the parental units </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "data shape torch.Size([67349, 40])\n",
            "label shape torch.Size([67349])\n",
            "mask shape torch.Size([67349, 40])\n",
            "tok type id shape torch.Size([67349, 40])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhKnU0-x5flb",
        "outputId": "9529fe63-770d-4090-e514-c0b81aa3790c"
      },
      "source": [
        "bert_model = RobertaForSequenceClassification.from_pretrained(\r\n",
        "    'roberta-base',\r\n",
        "    num_labels=2,\r\n",
        ")\r\n",
        "bert_model = bert_model.to(device)\r\n",
        "optimizer = AdamW(bert_model.parameters(), lr=4e-5, eps=1e-10)\r\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNPGgT3o5ni9",
        "outputId": "59bab394-60db-4f5e-c78f-45f525cf614f"
      },
      "source": [
        "###  train model which will score examples --> SCORING FUNCTION ##\r\n",
        "\r\n",
        "_ = train_model(bert_model, optimizer, epochs=4, loader=train_dataloader)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss after 1 epochs is 0.261697 \n",
            "train loss after 2 epochs is 0.181568 \n",
            "train loss after 3 epochs is 0.154203 \n",
            "train loss after 4 epochs is 0.133595 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQTHEY2q5vL1",
        "outputId": "e76a7c5d-8a50-46af-bac6-1f9db9088457"
      },
      "source": [
        "###   prepare train data to get loss om each  ###\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_train, model=\"bert\")\r\n",
        "train_dataloader = dataloader(ids, masks, tok_type_ids, y_train, \"val\", batch_size=1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<s>hide new secretions from the parental units </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "data shape torch.Size([67349, 40])\n",
            "label shape torch.Size([67349])\n",
            "mask shape torch.Size([67349, 40])\n",
            "tok type id shape torch.Size([67349, 40])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlLhCRIk5y3C"
      },
      "source": [
        "#####   Approach  3   get loss on all training data   #####\r\n",
        "\r\n",
        "bert_model.eval()\r\n",
        "loss_list = []\r\n",
        "for i, data in enumerate(train_dataloader):\r\n",
        "    batch_data = data[0].to(device).long()\r\n",
        "    batch_mask = data[1].to(device).long()\r\n",
        "    batch_labels = data[3].to(device).long()\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        pred = bert_model(batch_data,\r\n",
        "                                token_type_ids=None,\r\n",
        "                                attention_mask=batch_mask,\r\n",
        "                                labels=batch_labels)\r\n",
        "        \r\n",
        "        \r\n",
        "    loss = pred[0]\r\n",
        "    loss_list.append(loss.item())\r\n",
        "    "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3DK8YFx569Z",
        "outputId": "c1c9da77-584b-41fe-a6eb-62aa3895d9c0"
      },
      "source": [
        "########     SORT EXAMPLES BASED ON TOUGHNESS     ######\r\n",
        "\r\n",
        "print(max(loss_list))\r\n",
        "print(min(loss_list))\r\n",
        "\r\n",
        "train_loss = []\r\n",
        "\r\n",
        "for k in range(len(x_train)):\r\n",
        "    train_loss.append((x_train[k], y_train[k], loss_list[k]))\r\n",
        "    # label_loss.append((y_train[k], loss_list[k]))\r\n",
        "\r\n",
        "print(train_loss[-1])\r\n",
        "train_loss = sorted(train_loss, key = lambda x: x[2])\r\n",
        "\r\n",
        "print(train_loss[-1])\r\n",
        "\r\n",
        "x_train_sorted = []\r\n",
        "y_train_sorted = []\r\n",
        "\r\n",
        "for k in train_loss:\r\n",
        "    x_train_sorted.append(k[0])\r\n",
        "    y_train_sorted.append(k[1])\r\n",
        "\r\n",
        "print(x_train_sorted[-1])\r\n",
        "print(len(x_train_sorted), len(y_train_sorted))\r\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.526257038116455\n",
            "0.010813566856086254\n",
            "('this new jangle of noise , mayhem and stupidity must be a serious contender for the title . ', 0, 0.011442981660366058)\n",
            "('a damn fine and a truly distinctive and a deeply pertinent film ', 0, 4.526257038116455)\n",
            "a damn fine and a truly distinctive and a deeply pertinent film \n",
            "67349 67349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DmVwtvP5_wa",
        "outputId": "268f46bb-1876-4aa7-dcf4-0427689cd588"
      },
      "source": [
        "####   NOW Preapare data in curriculum order from sorted examples  ####\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_train_sorted, model=\"bert\")\r\n",
        "train_dataloader = dataloader(ids, masks, tok_type_ids, y_train_sorted, \"train\", batch_size=32)\r\n",
        "\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_val, model=\"bert\")\r\n",
        "val_dataloader = dataloader(ids, masks, tok_type_ids, y_val, \"val\", batch_size=32)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<s>bigelow demonstrates a breadth of vision and an attention to detail that propels her into the upper echelons of the directing world. </s><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "data shape torch.Size([67349, 40])\n",
            "label shape torch.Size([67349])\n",
            "mask shape torch.Size([67349, 40])\n",
            "tok type id shape torch.Size([67349, 40])\n",
            "<s>it's a charming and often affecting journey. </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "data shape torch.Size([872, 40])\n",
            "label shape torch.Size([872])\n",
            "mask shape torch.Size([872, 40])\n",
            "tok type id shape torch.Size([872, 40])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLAHeA9N6PnI",
        "outputId": "57ac10f4-b31d-4952-bafb-67017aa4ac78"
      },
      "source": [
        "###  model for training in curriculum oreder  ###\r\n",
        "\r\n",
        "new_model = RobertaForSequenceClassification.from_pretrained(\r\n",
        "    \"roberta-base\",\r\n",
        "    num_labels=2,\r\n",
        "    output_attentions = False,\r\n",
        "    output_hidden_states = False,\r\n",
        ")\r\n",
        "\r\n",
        "new_model = new_model.to(device)\r\n",
        "optim = AdamW(new_model.parameters(), lr=4e-5, eps=1e-8)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywNR3NTj6T8D",
        "outputId": "93cd6065-9611-4cc5-de02-7cbfd6bcd3a1"
      },
      "source": [
        "###  train model  IN Curriculum ORDER FOR 2 epochs then in RANDOM ORDER  ####\r\n",
        "\r\n",
        "arr = train_model(new_model, optim, 2, train_dataloader)\r\n",
        "ids, masks, tok_type_ids = get_ids_and_mask(tokenizer=tokenizer_bert, x=x_train, model=\"bert\")\r\n",
        "train_dataloader = dataloader(ids, masks, tok_type_ids, y_train, \"train\", batch_size=32)\r\n",
        "arr = train_model(new_model, optim, 2, train_dataloader)\r\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss after 1 epochs is 0.269245 \n",
            "train loss after 2 epochs is 0.180205 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<s>hide new secretions from the parental units </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "data shape torch.Size([67349, 40])\n",
            "label shape torch.Size([67349])\n",
            "mask shape torch.Size([67349, 40])\n",
            "tok type id shape torch.Size([67349, 40])\n",
            "train loss after 1 epochs is 0.156425 \n",
            "train loss after 2 epochs is 0.142541 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TijjN8oX6ljn",
        "outputId": "140a7cc3-4583-4f53-f48a-67deea37bb48"
      },
      "source": [
        "validate_model(new_model, val_dataloader)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final test set accuracy is  0.9105504587155964\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}